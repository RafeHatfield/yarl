# Test Cleanup Strategy - Quality Over Quantity

**Date:** October 20, 2025  
**Current Status:** 2,155 passing (90.5%), 112 failing, 117 quarantined  
**Goal:** 100% passing with **valuable, maintainable tests**

---

## Philosophy: Value > Coverage

**Principle:** A smaller suite of **robust, valuable tests** is better than a large suite with brittle, low-value tests.

### What Makes a Test Valuable?

âœ… **Keep Tests That:**
- Test core game mechanics (combat, movement, inventory)
- Test user-facing behavior (not implementation details)
- Are simple and maintainable
- Would catch real bugs
- Document expected behavior clearly

âŒ **Delete Tests That:**
- Test implementation details (internal method calls, mock assertions)
- Are brittle (break when internals change)
- Test old bugs that are long fixed (unless critical)
- Require complex mocking that's hard to maintain
- Test functionality that no longer exists

---

## Current Test Categories - Evaluation

### 1. **Regression Tests** (13 files, 61 tests)
**Status:** 54 passing, 7 failing  
**Value Assessment:** **MIXED - Needs Cleanup**

**Issues:**
- Many test implementation details (`assert_called`, mock assertions)
- Testing old bugs from months/years ago
- Brittle mocking that breaks with refactoring

**Action:**
- âœ… **Keep:** Tests for critical bugs (save/load corruption, data loss)
- âŒ **Delete:** Tests checking internal method calls
- ğŸ”§ **Rewrite:** Important functionality tests to check behavior

**Example - DELETE:**
```python
# BAD: Testing implementation detail
def test_shield_equipping_regression(self):
    self.player.equipment.toggle_equip(shield)
    self.state_manager.state.message_log.add_message.assert_called()  # âŒ Internal detail
```

**Example - KEEP/REWRITE:**
```python
# GOOD: Testing behavior
def test_shield_equipping(self):
    self.player.equipment.toggle_equip(shield)
    assert self.player.equipment.off_hand == shield  # âœ… User-visible behavior
    assert shield.equippable.owner == self.player
```

---

### 2. **AI System Tests** (3 files, ~20 tests)
**Status:** Multiple failures  
**Value Assessment:** **MEDIUM - Architecture Changed**

**Issues:**
- Testing old ECS architecture
- Complex system-level mocking
- May not match current implementation

**Action:**
- ğŸ” **Investigate:** Does current AI work? If yes, delete old tests
- ğŸ”§ **Rewrite:** Key AI behaviors with simpler tests
- âŒ **Delete:** ECS-specific tests if architecture changed

---

### 3. **Combat System Tests** (test_base_damage_system, test_combat_debug_logging)
**Status:** 8 failing  
**Value Assessment:** **HIGH - But Brittle**

**Issues:**
- Heavy mocking of status_effects
- Testing with Mock objects instead of real entities

**Action:**
- ğŸ”§ **Fix:** These are valuable - combat is core
- ğŸ’¡ **Improve:** Use real entities, not mocks
- Pattern we used: `equipment = Equipment(player)` not `Equipment()`

---

### 4. **Difficulty Scaling Tests** (test_difficulty_scaling.py)
**Status:** 10 failing (StopIteration in mocks)  
**Value Assessment:** **MEDIUM - Outdated Mocking**

**Issues:**
- Mock iterators causing StopIteration
- Testing procedural generation details
- May be testing implementation, not behavior

**Action:**
- ğŸ” **Evaluate:** Is difficulty scaling working? Test it in-game
- âŒ **Delete:** If tests are just checking internal spawn logic
- ğŸ”§ **Rewrite:** If testing actual difficulty progression

---

### 5. **Quarantined Tests** (13 files, 117 tests)
**Status:** Skipped, unknown pass rate  
**Value Assessment:** **NEEDS REVIEW**

**Files:**
- Save/Load (2 files, 20 tests) - **HIGH VALUE**
- Integration tests (3 files, 26 tests) - **MEDIUM VALUE**
- Boss dialogue, pathfinding, etc. - **LOW-MEDIUM VALUE**

**Action:**
- ğŸ” **Triage:** Un-quarantine one by one
- âœ… **Fix:** High-value tests (save/load, core mechanics)
- âŒ **Delete:** Low-value or obsolete tests

---

## Recommended Deletion Candidates

### High Confidence Deletes (Estimate: 30-40 tests)

1. **Regression tests checking mock calls** (~10 tests)
   - `test_*_regression` files with `assert_called` checks
   - Replace with behavior tests if needed

2. **Old architecture tests** (~10 tests)
   - ECS system tests if we moved away from that
   - Old AI architecture tests

3. **Implementation detail tests** (~10 tests)
   - Tests checking internal state
   - Tests verifying private method calls

4. **Duplicate coverage** (~10 tests)
   - Tests that cover same functionality as other tests
   - Regression tests for bugs already caught by unit tests

### Medium Confidence Deletes (Estimate: 20-30 tests)

1. **Brittle integration tests** (~15 tests)
   - Tests requiring complex mock setups
   - Tests that break frequently with refactoring

2. **Low-value feature tests** (~10 tests)
   - Testing minor features
   - Features that may have changed/been removed

---

## Action Plan

### Phase 1: Quick Wins (Target: +30 tests, -20 tests)
**Goal:** Get to 95% by fixing easy tests and deleting obvious cruft

1. Fix remaining simple quarantined tests (healing, etc.) âœ…
2. Delete brittle regression tests checking mock calls
3. Delete obsolete architecture tests
4. **Result:** ~2,165 passing, ~80 failing/quarantined

### Phase 2: High-Value Fixes (Target: +40 tests)
**Goal:** Fix tests worth keeping

1. Fix combat system tests (use real entities)
2. Fix save/load tests (core functionality)
3. Rewrite key regression tests as behavior tests
4. **Result:** ~2,205 passing, ~40 failing/quarantined

### Phase 3: Final Cleanup (Target: 100%)
**Goal:** Delete remaining low-value tests

1. Delete remaining brittle tests
2. Delete tests for removed features
3. Delete duplicate coverage
4. **Result:** ~2,210 passing, 0 failing, 0 quarantined = 100%

---

## Success Metrics

**Not This:**
- âŒ "We have 2,400 tests!" (quantity)
- âŒ "100% code coverage!" (vanity metric)

**But This:**
- âœ… "All tests are valuable and maintainable"
- âœ… "Tests catch real bugs"
- âœ… "Test failures mean real problems"
- âœ… "Team can maintain test suite easily"

---

## Expected Final Numbers

| Metric | Current | Target |
|--------|---------|--------|
| Total Tests | 2,384 | ~2,210 |
| Passing | 2,155 (90.5%) | 2,210 (100%) |
| Deleted | 0 | ~170 |
| **Net Change** | - | +55 passing, -170 deleted |

**Key Insight:** We'll have **fewer but better tests** with 100% pass rate.

---

## Implementation Notes

### Deleting Tests

**DO:**
- Document why we're deleting (in commit message)
- Keep test if unsure - can delete later
- Look for patterns (delete whole categories)

**DON'T:**
- Delete without understanding what it tests
- Delete tests for core functionality
- Rush - take time to evaluate

### Commit Strategy

```bash
# Group deletions by category
git commit -m "ğŸ—‘ï¸ Delete: Brittle regression tests (mock call assertions)

Deleted 10 regression tests that check internal method calls
rather than user-visible behavior.

Rationale: These tests are brittle and break with refactoring
even when functionality works correctly. The bugs they caught
are long fixed and covered by unit tests.

Files affected:
- test_inventory_bugs_regression.py (4 tests)
- test_ai_system_regression.py (3 tests)
- test_render_system_regression.py (3 tests)"
```

---

## Next Session Plan

1. **Identify deletion candidates** (30 min)
   - Review failing tests
   - Mark for delete/fix/rewrite

2. **Delete obvious cruft** (30 min)
   - Brittle regression tests
   - Obsolete architecture tests

3. **Fix high-value tests** (60 min)
   - Combat system tests
   - Save/load tests

4. **Final sweep** (30 min)
   - Review remaining failures
   - Delete or fix

**Estimated Time:** 2.5 hours to 100%  
**Expected Outcome:** ~2,210 passing tests, all valuable and maintainable

---

**Remember:** We're building a **maintainable test suite**, not maximizing test count!

